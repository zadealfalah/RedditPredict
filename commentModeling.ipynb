{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will serve as the notebook for testing my topic modeling portion of this project.\n",
    "\n",
    "After verifying that everything is working, it will be changed to a .py file to be runnable on new incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "# from topicModelingFunctions import text_preproc\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>flag</th>\n",
       "      <th>cComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ain't no half steppin</td>\n",
       "      <td>okay</td>\n",
       "      <td>aint half steppin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Inside the gas tank cover or on top of a tire</td>\n",
       "      <td>okay</td>\n",
       "      <td>inside gas tank cover top tire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Brought to you by /r/SummerReddit</td>\n",
       "      <td>okay</td>\n",
       "      <td>brought rsummerreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This makes me want to murder.</td>\n",
       "      <td>okay</td>\n",
       "      <td>makes want murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>I know someone who got a DUI sleeping it off i...</td>\n",
       "      <td>okay</td>\n",
       "      <td>know someone got dui sleeping car saying cops ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               body  flag  \\\n",
       "0      1                              Ain't no half steppin  okay   \n",
       "1      7      Inside the gas tank cover or on top of a tire  okay   \n",
       "2     12                  Brought to you by /r/SummerReddit  okay   \n",
       "3      1                     This makes me want to murder.   okay   \n",
       "4      3  I know someone who got a DUI sleeping it off i...  okay   \n",
       "\n",
       "                                            cComment  \n",
       "0                                  aint half steppin  \n",
       "1                     inside gas tank cover top tire  \n",
       "2                              brought rsummerreddit  \n",
       "3                                  makes want murder  \n",
       "4  know someone got dui sleeping car saying cops ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/temp/2014_funny_cComments.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Spacy Text Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>flag</th>\n",
       "      <th>cComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ain't no half steppin</td>\n",
       "      <td>okay</td>\n",
       "      <td>aint half steppin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Inside the gas tank cover or on top of a tire</td>\n",
       "      <td>okay</td>\n",
       "      <td>inside gas tank cover top tire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Brought to you by /r/SummerReddit</td>\n",
       "      <td>okay</td>\n",
       "      <td>brought rsummerreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This makes me want to murder.</td>\n",
       "      <td>okay</td>\n",
       "      <td>makes want murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>I know someone who got a DUI sleeping it off i...</td>\n",
       "      <td>okay</td>\n",
       "      <td>know someone got dui sleeping car saying cops ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               body  flag  \\\n",
       "0      1                              Ain't no half steppin  okay   \n",
       "1      7      Inside the gas tank cover or on top of a tire  okay   \n",
       "2     12                  Brought to you by /r/SummerReddit  okay   \n",
       "3      1                     This makes me want to murder.   okay   \n",
       "4      3  I know someone who got a DUI sleeping it off i...  okay   \n",
       "\n",
       "                                            cComment  \n",
       "0                                  aint half steppin  \n",
       "1                     inside gas tank cover top tire  \n",
       "2                              brought rsummerreddit  \n",
       "3                                  makes want murder  \n",
       "4  know someone got dui sleeping car saying cops ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_json(\"data/temp/2014_funny_cComments.jsonl\", lines=True,nrows=5)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m testtup \u001b[39m=\u001b[39m ((comm[\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m], comm) \u001b[39mfor\u001b[39;00m comm \u001b[39min\u001b[39;00m test)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m doc, comm \u001b[39min\u001b[39;00m nlp\u001b[39m.\u001b[39mpipe(testtup, as_tuples\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(doc, comm)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\language.py:1539\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1529\u001b[0m docs_with_contexts \u001b[39m=\u001b[39m (\n\u001b[0;32m   1530\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_doc_with_context(text, context) \u001b[39mfor\u001b[39;00m text, context \u001b[39min\u001b[39;00m texts\n\u001b[0;32m   1531\u001b[0m )\n\u001b[0;32m   1532\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipe(\n\u001b[0;32m   1533\u001b[0m     docs_with_contexts,\n\u001b[0;32m   1534\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m     component_cfg\u001b[39m=\u001b[39mcomponent_cfg,\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m   1540\u001b[0m     context \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39m_context\n\u001b[0;32m   1541\u001b[0m     doc\u001b[39m.\u001b[39m_context \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\language.py:1583\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1581\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[0;32m   1582\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n\u001b[1;32m-> 1583\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1611\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1604\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1605\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1609\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1610\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1611\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1611\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1604\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1605\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1609\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1610\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1611\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1611\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1604\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1605\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1609\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1610\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1611\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1560\u001b[0m, in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1559\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(size_)\n\u001b[1;32m-> 1560\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(items, \u001b[39mint\u001b[39;49m(batch_size)))\n\u001b[0;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1562\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1611\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1604\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1605\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1609\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1610\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1611\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\util.py:1560\u001b[0m, in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1559\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(size_)\n\u001b[1;32m-> 1560\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(items, \u001b[39mint\u001b[39;49m(batch_size)))\n\u001b[0;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1562\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\language.py:1580\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001b[0;32m   1578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1579\u001b[0m     \u001b[39m# if n_process == 1, no processes are forked.\u001b[39;00m\n\u001b[1;32m-> 1580\u001b[0m     docs \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_doc(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts)\n\u001b[0;32m   1581\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[0;32m   1582\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py1010-red\\lib\\site-packages\\spacy\\language.py:1529\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[39mif\u001b[39;00m as_tuples:\n\u001b[0;32m   1528\u001b[0m     texts \u001b[39m=\u001b[39m cast(Iterable[Tuple[Union[\u001b[39mstr\u001b[39m, Doc], _AnyContext]], texts)\n\u001b[1;32m-> 1529\u001b[0m     docs_with_contexts \u001b[39m=\u001b[39m (\n\u001b[0;32m   1530\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_doc_with_context(text, context) \u001b[39mfor\u001b[39;00m text, context \u001b[39min\u001b[39;00m texts\n\u001b[0;32m   1531\u001b[0m     )\n\u001b[0;32m   1532\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipe(\n\u001b[0;32m   1533\u001b[0m         docs_with_contexts,\n\u001b[0;32m   1534\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         component_cfg\u001b[39m=\u001b[39mcomponent_cfg,\n\u001b[0;32m   1538\u001b[0m     )\n\u001b[0;32m   1539\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testtup \u001b[39m=\u001b[39m ((comm[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m], comm) \u001b[39mfor\u001b[39;00m comm \u001b[39min\u001b[39;00m test)\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m doc, comm \u001b[39min\u001b[39;00m nlp\u001b[39m.\u001b[39mpipe(testtup, as_tuples\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(doc, comm)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "testtup = ((comm['body'], comm) for comm in test)\n",
    "for doc, comm in nlp.pipe(testtup, as_tuples=True):\n",
    "    print(doc, comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = np.unique(df.flag)\n",
    "# values = class_weight.compute_class_weight(class_weight='balanced', classes=keys, y=df.flag)\n",
    "# class_weights = dict(zip(keys, values))\n",
    "# class_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has no built-in functions to handle imbalanced data.  Should do subsampling or something of the sort here before selecting training/testing/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cComment, df.flag, test_size=0.15)  \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)  # Create training, testing, and validation splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224999054063929\n",
      "0.15000004332531622\n",
      "0.12750005126829086\n"
     ]
    }
   ],
   "source": [
    "# Check %ages of splits\n",
    "print(len(X_train)/len(df))\n",
    "print(len(X_test)/len(df))\n",
    "print(len(X_val)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupVal = tuple(zip(X_val, y_val))\n",
    "tupTrain = tuple(zip(X_train, y_train))\n",
    "tupTest = tuple(zip(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is far too large to store locally, change make_docs to be a stream of data ala:\n",
    "\n",
    "https://spacy.io/usage/training#custom-code-readers-batchers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(tupleXY):\n",
    "    \"\"\"\n",
    "    Function to transform our data into docs for spacy\n",
    "\n",
    "    Args:\n",
    "        data \n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for doc, label in nlp.pipe(tupleXY, as_tuples=True):\n",
    "        if label == 'okay':\n",
    "            doc.cats['okay'] = 1\n",
    "            doc.cats['bad'] = 0\n",
    "            doc.cats['good'] = 0\n",
    "            \n",
    "        elif label == 'bad':\n",
    "            doc.cats['okay'] = 0\n",
    "            doc.cats['bad'] = 1\n",
    "            doc.cats['good'] = 0\n",
    "            \n",
    "        else:\n",
    "            doc.cats['okay'] = 0\n",
    "            doc.cats['bad'] = 0\n",
    "            doc.cats['good'] = 1\n",
    "        \n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Making docs\")\n",
    "# trainDocs = make_docs(tupTrain)\n",
    "# testDocs = make_docs(tupTest)\n",
    "# valDocs = make_docs(tupVal)\n",
    "\n",
    "# print(\"Starting DocBins\")\n",
    "# docBinTrain = DocBin(docs=trainDocs)\n",
    "# docBinTrain.to_disk(r\"data/models/spacy/train.spacy\")\n",
    "# print(\"Finished Training Doc Bins\")\n",
    "# docBinTest = DocBin(docs=testDocs)\n",
    "# docBinTest.to_disk(r\"data/models/spacy/test.spacy\")\n",
    "# print(\"Finished Testing Doc Bins\")\n",
    "# docBinVal = DocBin(docs=valDocs)\n",
    "# docBinVal.to_disk(r\"data/models/spacy/valid.spacy\")\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer at 0x2cc76e25ca0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.5,\n",
    "    \"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "}\n",
    "\n",
    "nlp.add_pipe(\"textcat_multilabel\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start BERT modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)  # tfhub_handle_preprocess from the \"Choose a BERT model to fine-tune\" code at https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)  # tfhub_handle_encoder from the \"Choose a BERT model to fine-tune\" code at https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='body')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model =build_Classifier_model()\n",
    "tf.keras.utils.plot_model(classifier_model)  # Check model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Multi-class classification\n",
    "metrics = tf.metrics.Accuracy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()  ##### Must change to our actual training data instead of train_ds\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)  # first 10% of training steps to warmup optimizer\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BERT model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "print(f\"Training model with {tfhub_handle_encoder}\")\n",
    "history = classifier_model.fit(x=train_ds,  #### Must change to actual training data\n",
    "                               validation_data=val_ds,   #### Must change to actual validation data\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)   #### Must change to actual test data\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"redditPredict\"\n",
    "saved_model_path = './data/models/{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py0913-red",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
