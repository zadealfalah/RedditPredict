{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will serve as the notebook for testing my topic modeling portion of this project.\n",
    "\n",
    "After verifying that everything is working, it will be changed to a .py file to be runnable on new incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "# from topicModelingFunctions import text_preproc\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>flag</th>\n",
       "      <th>cComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ain't no half steppin</td>\n",
       "      <td>okay</td>\n",
       "      <td>aint half steppin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Inside the gas tank cover or on top of a tire</td>\n",
       "      <td>okay</td>\n",
       "      <td>inside gas tank cover top tire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Brought to you by /r/SummerReddit</td>\n",
       "      <td>okay</td>\n",
       "      <td>brought rsummerreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This makes me want to murder.</td>\n",
       "      <td>okay</td>\n",
       "      <td>makes want murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>I know someone who got a DUI sleeping it off i...</td>\n",
       "      <td>okay</td>\n",
       "      <td>know someone got dui sleeping car saying cops ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               body  flag  \\\n",
       "0      1                              Ain't no half steppin  okay   \n",
       "1      7      Inside the gas tank cover or on top of a tire  okay   \n",
       "2     12                  Brought to you by /r/SummerReddit  okay   \n",
       "3      1                     This makes me want to murder.   okay   \n",
       "4      3  I know someone who got a DUI sleeping it off i...  okay   \n",
       "\n",
       "                                            cComment  \n",
       "0                                  aint half steppin  \n",
       "1                     inside gas tank cover top tire  \n",
       "2                              brought rsummerreddit  \n",
       "3                                  makes want murder  \n",
       "4  know someone got dui sleeping car saying cops ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"data/temp/2014_funny_cComments.csv\", usecols=['score', 'body', 'flag', 'cComment'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Spacy Text Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.cComment.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = np.unique(df.flag)\n",
    "# values = class_weight.compute_class_weight(class_weight='balanced', classes=keys, y=df.flag)\n",
    "# class_weights = dict(zip(keys, values))\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.cComment, df.flag, test_size=0.15)  \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)  # Create training, testing, and validation splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224999054063929\n",
      "0.15000004332531622\n",
      "0.12750005126829086\n"
     ]
    }
   ],
   "source": [
    "# Check %ages of splits\n",
    "print(len(X_train)/len(df))\n",
    "print(len(X_test)/len(df))\n",
    "print(len(X_val)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipVal = zip(X_val, y_val)\n",
    "zipTrain = zip(X_train, y_train)\n",
    "zipTest = zip(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(zippedList, outfile):\n",
    "    \"\"\"\n",
    "    Function to transform our data into docs for spacy\n",
    "\n",
    "    Args:\n",
    "        data (zip): zipped list of sentences and categories to transform\n",
    "        outfile (str): path to save the model to\n",
    "    \"\"\"\n",
    "    db = DocBin()\n",
    "    for comment, label in zippedList:\n",
    "        # print(comment, label)\n",
    "        doc = nlp.make_doc(comment)\n",
    "        if label == 'okay':\n",
    "            doc.cats['okay'] = 1\n",
    "            doc.cats['bad'] = 0\n",
    "            doc.cats['good'] = 0\n",
    "            \n",
    "        elif label == 'bad':\n",
    "            doc.cats['okay'] = 0\n",
    "            doc.cats['bad'] = 1\n",
    "            doc.cats['good'] = 0\n",
    "            \n",
    "        else:\n",
    "            doc.cats['okay'] = 0\n",
    "            doc.cats['bad'] = 0\n",
    "            doc.cats['good'] = 1\n",
    "        \n",
    "        db.add(doc)\n",
    "    db.to_disk(outfile)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Finished Val\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.52 GiB for an array with shape (77645270, 13) and data type uint64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\mastersCoursework\\Data Mining Foundations and Practice (5504, 5505, 5506)\\5506\\RedditPredict\\commentModeling.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m make_docs(zipVal, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/models/spacy/valid.spacy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished Val\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m make_docs(zipTrain, \u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/models/spacy/train.spacy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished Train\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m make_docs(zipTest, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/models/spacy/test.spacy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\mastersCoursework\\Data Mining Foundations and Practice (5504, 5505, 5506)\\5506\\RedditPredict\\commentModeling.ipynb Cell 12\u001b[0m in \u001b[0;36mmake_docs\u001b[1;34m(zippedList, outfile)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         doc\u001b[39m.\u001b[39mcats[\u001b[39m'\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     db\u001b[39m.\u001b[39madd(doc)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/mastersCoursework/Data%20Mining%20Foundations%20and%20Practice%20%285504%2C%205505%2C%205506%29/5506/RedditPredict/commentModeling.ipynb#X63sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m db\u001b[39m.\u001b[39;49mto_disk(outfile)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py0913\\lib\\site-packages\\spacy\\tokens\\_serialize.py:260\u001b[0m, in \u001b[0;36mDocBin.to_disk\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m path\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_:\n\u001b[0;32m    259\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m         file_\u001b[39m.\u001b[39mwrite(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_bytes())\n\u001b[0;32m    261\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE870)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py0913\\lib\\site-packages\\spacy\\tokens\\_serialize.py:200\u001b[0m, in \u001b[0;36mDocBin.to_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tokens\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, tokens\u001b[39m.\u001b[39mshape  \u001b[39m# this should never happen\u001b[39;00m\n\u001b[0;32m    199\u001b[0m lengths \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(tokens) \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens]\n\u001b[1;32m--> 200\u001b[0m tokens \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mvstack(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens \u001b[39melse\u001b[39;00m numpy\u001b[39m.\u001b[39masarray([])\n\u001b[0;32m    201\u001b[0m spaces \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspaces) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspaces \u001b[39melse\u001b[39;00m numpy\u001b[39m.\u001b[39masarray([])\n\u001b[0;32m    202\u001b[0m msg \u001b[39m=\u001b[39m {\n\u001b[0;32m    203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion,\n\u001b[0;32m    204\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattrs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mspan_groups\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_groups,\n\u001b[0;32m    212\u001b[0m }\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\anaconda3\\envs\\py0913\\lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.52 GiB for an array with shape (77645270, 13) and data type uint64"
     ]
    }
   ],
   "source": [
    "print(\"Starting\")\n",
    "make_docs(zipVal, r\"data/models/spacy/valid.spacy\")\n",
    "print(\"Finished Val\")\n",
    "make_docs(zipTrain, r\"data/models/spacy/train.spacy\")\n",
    "print(\"Finished Train\")\n",
    "make_docs(zipTest, r\"data/models/spacy/test.spacy\")\n",
    "print(\"Finished Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer at 0x2cc76e25ca0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.5,\n",
    "    \"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "}\n",
    "\n",
    "nlp.add_pipe(\"textcat_multilabel\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start BERT modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)  # tfhub_handle_preprocess from the \"Choose a BERT model to fine-tune\" code at https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)  # tfhub_handle_encoder from the \"Choose a BERT model to fine-tune\" code at https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='body')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model =build_Classifier_model()\n",
    "tf.keras.utils.plot_model(classifier_model)  # Check model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Multi-class classification\n",
    "metrics = tf.metrics.Accuracy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()  ##### Must change to our actual training data instead of train_ds\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)  # first 10% of training steps to warmup optimizer\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BERT model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "print(f\"Training model with {tfhub_handle_encoder}\")\n",
    "history = classifier_model.fit(x=train_ds,  #### Must change to actual training data\n",
    "                               validation_data=val_ds,   #### Must change to actual validation data\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)   #### Must change to actual test data\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"redditPredict\"\n",
    "saved_model_path = './data/models/{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py0913-red",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
