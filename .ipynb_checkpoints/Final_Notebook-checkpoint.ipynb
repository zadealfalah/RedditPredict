{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import os\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') #required for lemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation, Input, BatchNormalization, MaxPooling1D, LSTM, Bidirectional, CategoryEncoding\n",
    "from keras.layers.core import Masking, Dropout, Reshape\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37f06f",
   "metadata": {},
   "source": [
    "# Reddit Comment Karma Predictions\n",
    "# Project Overview  \n",
    "This project came as the result of attempting to create a regression model to predict the score a given comment might get on a reddit submission.  The original scope was to hopefully build up a regression model for an individual subreddit, then it shrunk to being for a smaller timeframe within an individual subreddit, and finally it ended with this binary 'good' vs. 'bad' posting problem where we label comments with a score less than or equal to one as being a 'bad' comment, and ones with a score over one to be a 'good' comment.  This was decided because comments that are not interacted with in any way remain at one point, and as such it would be hard to call it a 'good' comment if it garners no interaction at all.  So in the end, this became transformed in to a binary classification problem.  We will attempt to model this project via CNNs and RNNs and will assess model performance based on the following: Validation Accuracy, Validation AUC, Time to Fit, and Loss Curves.  We will also do a brief comparison to a non Deep Learning model.  It is assumed that RNNs will perform the best.  \n",
    "There are a myriad of reasons why this type of model might be useful to others including, but not limited to, businesses looking to create content within a given subreddit that would be received well by those who see it - or to promote it further so it is seen by more.  Perhaps they have an ad which is being interacted with already and they wish to interact further with members of the community within the comments of the thread.\n",
    "While obviously a binary 'good' vs. 'bad' is not quite helpful on its own, it can serve as a bedrock from which further analysis can be done with more depth.  The approach of going subreddit-by-subreddit seems to have been skipped over by many others in an attempt to get some sort of 'best predictor' in general which seemed overly ambitious as I doubt that all subreddits have identical preferences and as such their individuality must be taken in to account within our predictors.    \n",
    "The data for this was gathered via the Pushshift API which allows for us to customize our selection criteria for comments to be pulled.  In this analysis the largest section of data that we looked at pulled a total of 1,200,000 comments over the course of 2020 from the 'funny' subreddit.  The comments were pulled along with their unique id, the author's name, the time it was created, the subreddit it was from, and the score it recieved.  \n",
    "The data was pulled month-by-month in an attempt to spread out our data collection in a way that would allow for some time series analyses, but as will be shown later in the notebook, this failed due to the way the API chooses the comments it selects which was not realized in time.  The pull requests also specified the scores to pull from in an attempt to balance the 'good' and 'bad' classes to an extent.  This may impact the model performance but there was a worry of severely undersampling the 'bad' class that outweighed this concern.  \n",
    "<br />\n",
    "\n",
    "I would also like to mention that when my original ideas didn't pan out as I wanted, I was inspired by Max Woolf's 'Predicting the Success of a Reddit Submission with Deep Learning and Keras' notebook.  Part of my original reason for including date information was due to this notebook, and in fact the model that will later be referred to as 'original' refers to the model in this notebook which was used as a point of comparison.\n",
    "\n",
    "<br />\n",
    "\n",
    "Pushshift API link: https://reddit-api.readthedocs.io/en/latest/  \n",
    "\n",
    "Max Woolf's Notebook: https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5efd65",
   "metadata": {},
   "source": [
    "## Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439eb7d",
   "metadata": {},
   "source": [
    "Here we set up the timestamps, month-by-month covering 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Init the timestamps (start and end) for consideration\n",
    "# stimes = [int(dt.datetime(2020, 1, 1).timestamp()), int(dt.datetime(2020, 2, 1).timestamp()),\n",
    "#          int(dt.datetime(2020, 3, 1).timestamp()), int(dt.datetime(2020, 4, 1).timestamp()),\n",
    "#          int(dt.datetime(2020, 5, 1).timestamp()), int(dt.datetime(2020, 6, 1).timestamp()),\n",
    "#          int(dt.datetime(2020, 7, 1).timestamp()), int(dt.datetime(2020, 8, 1).timestamp()),\n",
    "#          int(dt.datetime(2020, 9, 1).timestamp()), int(dt.datetime(2020, 10, 1).timestamp()),\n",
    "#          int(dt.datetime(2020, 11, 1).timestamp()), int(dt.datetime(2020, 12, 1).timestamp())]\n",
    "\n",
    "# etimes = [int(dt.datetime(2020, 1, 31).timestamp()), int(dt.datetime(2020, 2, 29).timestamp())\n",
    "#          int(dt.datetime(2020, 3, 31).timestamp()), int(dt.datetime(2020, 4, 30).timestamp())\n",
    "#          int(dt.datetime(2020, 5, 31).timestamp()), int(dt.datetime(2020, 6, 30).timestamp()),\n",
    "#          int(dt.datetime(2020, 7, 31).timestamp()), int(dt.datetime(2020, 8, 31).timestamp()),\n",
    "#          int(dt.datetime(2020, 9, 30).timestamp()), int(dt.datetime(2020, 10, 31).timestamp()),\n",
    "#          int(dt.datetime(2020, 11, 30).timestamp()), int(dt.datetime(2020, 12, 31).timestamp())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9eac3",
   "metadata": {},
   "source": [
    "Here we set up the Pushshift API retreival function we use which specifies the details we pull out of the comments that we pull as well as defining which comments we pull from in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c22000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prep the Pushshift API retreival function\n",
    "# api = PushshiftAPI()\n",
    "# def data_prep_comments(subr, start_time, end_time, filters, limit, scorestr):\n",
    "#     if (len(filters) == 0):\n",
    "#         filters = ['id', 'author', 'created_utc', 'body', 'subreddit', 'score']\n",
    "#                    #We set by default some usefull columns \n",
    "\n",
    "#     comments = list(api.search_comments(\n",
    "#         subreddit = subr,       #Subreddit we want to audit\n",
    "#         after=start_time,       #Start date\n",
    "#         before=end_time,        #End date\n",
    "#         filter=filters,         #Column names we want to retrieve\n",
    "#         score = scorestr,         #set score limits\n",
    "#         limit=limit))           #Max number of comments\n",
    "#     return pd.DataFrame(comments) #Return dataframe for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc642c",
   "metadata": {},
   "source": [
    "Our actual pulls are commented out below showing how the data was stored initially.  There are three cells, each showing one of the score requirements we set in an attempt to balance our dataset.  Most comments have a score of 1 so the 'bad' class was split in half, half of which required the score to be below 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sOI_list = ['funny'] #had more than just 'funny' before, narrowed down to this\n",
    "# filters = [] #keep all features defaulted for now\n",
    "# limit = 50000 #max num. comments to retrieve for scorestr > 1 \n",
    "# for sOI in sOI_list:\n",
    "#     print(f\"Starting {sOI} Reading\")\n",
    "#     for i in range(len(stimes)):\n",
    "#         print(f\"Doing {dt.date.fromtimestamp(stimes[i])} to {dt.date.fromtimestamp(etimes[i])}\")\n",
    "#         testdf = data_prep_comments(sOI, stimes[i], etimes[i], filters, limit, scorestr = '>1')\n",
    "#         testdf.to_csv(f\"Comment_Data_Raw/{dt.date.fromtimestamp(stimes[i]).month}_over1_{sOI}_comments_numof_{limit}_{stimes[i]}_to_{etimes[i]}.csv\", index = False) #save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d98962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sOI_list = ['funny']\n",
    "# filters = [] #keep all features for now\n",
    "# limit = 25000 #max num. comments to retrieve for scorestr = 1\n",
    "# for sOI in sOI_list:\n",
    "#     print(f\"Starting {sOI} Reading\")\n",
    "#     for i in range(len(stimes)):\n",
    "#         print(f\"Doing {dt.date.fromtimestamp(stimes[i])} to {dt.date.fromtimestamp(etimes[i])}\")\n",
    "#         testdf = data_prep_comments(sOI, stimes[i], etimes[i], filters, limit, scorestr = '=1')\n",
    "#         testdf.to_csv(f\"Comment_Data_Raw/{dt.date.fromtimestamp(stimes[i]).month}_equal1_{sOI}_comments_numof_{limit}_{stimes[i]}_to_{etimes[i]}.csv\", index = False) #save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sOI_list = ['funny']\n",
    "# filters = [] #keep all features for now\n",
    "# limit = 25000 #max num. comments to retrieve for scorestr < 1\n",
    "# for sOI in sOI_list:\n",
    "#     print(f\"Starting {sOI} Reading\")\n",
    "#     for i in range(len(stimes)):\n",
    "#         print(f\"Doing {dt.date.fromtimestamp(stimes[i])} to {dt.date.fromtimestamp(etimes[i])}\")\n",
    "#         testdf = data_prep_comments(sOI, stimes[i], etimes[i], filters, limit, scorestr = '<1')\n",
    "#         testdf.to_csv(f\"Comment_Data_Raw/{dt.date.fromtimestamp(stimes[i]).month}_under1_{sOI}_comments_numof_{limit}_{stimes[i]}_to_{etimes[i]}.csv\", index = False) #save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Already saved my 2020 data, read it in here\n",
    "df = pd.read_csv(\"funny_2020_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0406852",
   "metadata": {},
   "source": [
    "## About the dataframe  \n",
    "Because it takes quite a while to do all this pulling the final output was concatenated and saved in to 'funny_2020_cleaned.csv' and read in above.  With that said, the process of how the raw data was transformed in to the above is listed below in the 'Cleaning the Data' section.  \n",
    "The full dataset is a (1054303, 19) dataframe.  \n",
    "The columns are listed blow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5680a32",
   "metadata": {},
   "source": [
    "The author, body, created_utc, id, score, and subreddit information were all taken from the API pull request.  The other features were created based off of these original ones.  The process of creating these additional features are shown in the following section(s) of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2abb4c",
   "metadata": {},
   "source": [
    "While there were many features, only a few were used in our analysis below.  Here are some statistics for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['score', 'BinaryBucket', 'SubjectivityBucket', 'PolarityBucket', 'LengthBucket',\n",
    "   'Hour', 'Weekday', 'Day_of_Year']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc909327",
   "metadata": {},
   "source": [
    "## Cleaning the Data  \n",
    "#### (Note that the text embeddings happen later, in the 'Modeling' section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3647f",
   "metadata": {},
   "source": [
    "This function takes the dataframe, removes 'AutoMod', 'deleted', and 'removed' posts as well as any duplicates that may have been pulled before it adds the 'Date', 'Hour', 'Weekday', 'Day_of_Year', and 'BinaryBucket' columns and dropping the 'created' and 'd_' columns which we will not need again.  \n",
    "The BinaryBucket column is our target variable for this project which has a 1 if the score for a given comment has a score over one ('good') and a 0 if it is less than or equal to 1 ('bad')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_df(df):\n",
    "    df = df[df.author != 'AutoModerator'] #get rid of automod posts\n",
    "    df.drop_duplicates(inplace = True) #there will be duplicates from the way we got our data, must remove them\n",
    "    df = df[(df['body'] != \"[deleted]\") & (df['body'] != '[removed]')] #remove [deleted] comments and [removed] comments\n",
    "    df['Date'] = df.created_utc.apply(dt.date.fromtimestamp) #get a 'date' column for easier reading.  \n",
    "    df.drop(labels = ['created', 'd_'], axis = 1, inplace = True) #drop two that we won't be using\n",
    "    df['BinaryBucket'] = np.where(df.score>1, 1, 0) #when score >1, it's a 1, else it's a 0\n",
    "    \n",
    "    df['Hour'] = df.created_utc.apply(lambda x: dt.datetime.fromtimestamp(x).hour) #get the hour column\n",
    "    df['Weekday'] = df.created_utc.apply(dt.date.fromtimestamp).apply(dt.date.weekday) #get the day of week column\n",
    "    df['Day_of_Year'] = df.Date.apply(lambda x: dt.date.timetuple(x).tm_yday) #get the day of year column\n",
    "    \n",
    "    \n",
    "    df.reset_index(drop = True, inplace = True) #reset index since we did a lot of moving things around\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacbd90",
   "metadata": {},
   "source": [
    "This cell initializes a lemmatizer then does some basic text preprocessing including removing whitespace, lowercasing, removing stopwords, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic text preprocessor\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def text_preproc(row):\n",
    "    text = str(row['body'])\n",
    "    text = text.encode('ascii', errors = 'ignore').decode() #remove non-ascii characters\n",
    "    text = re.sub(r'\\d+', '', text) #remove numbers\n",
    "    text = text.lower() #set to lowercase\n",
    "    text = text.strip() #remove whitespace\n",
    "    text = re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~.\\'!?]+','',text) #remove all punctuation \n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE) #remove URLs\n",
    "    \n",
    "    stops = set(stopwords.words('english')) #set of stopwords to compare to\n",
    "    #following line does quite a bit.  takes words from text, and if they are longer than 1 character and not a stop word then they join them back together\n",
    "    #using the lemmatized versions of the words in to a new string\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if (word not in stops) and len(word) > 1])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f7d74",
   "metadata": {},
   "source": [
    "This cell removed any comments which got 'completely cleaned away'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['cComment'] = df.apply(text_preproc, axis = 1)\n",
    "# df = df[~df.cComment.apply(lambda x: len(x.split()) == 0)]\n",
    "# df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d62b55",
   "metadata": {},
   "source": [
    "Here we have a basic function which counts the number of words in a given row.  It returned an error if the cell was, say, blank.  These errors were removed before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a900b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_words(txt):\n",
    "    try:\n",
    "        return(len(re.findall(r'[A-Za-z\\']+(?:\\`[A-Za-z]+)?', txt)))\n",
    "    except:\n",
    "        return(f\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2892b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['BodyLen'] = df.body.apply(find_num_words)\n",
    "# df[df.BodyLen == 'Error'] #remove those posts which had no body length. \n",
    "# could move above line to clean_up_df() but this is just to show how I did it, would move in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c4911",
   "metadata": {},
   "source": [
    "Here we used Textblob to create a 'Subjectivity' and 'Polarity' column within our dataframe, based upon the full text column.  This was inspired by the fact that part of my original project included some sentiment analysis.  While it was too time consuming to do a thorough sentiment analysis in addition to the other work for this project, this was brought in to see if it could have any impact as a feature in leiu of a true sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1542f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['BodySubjectivity'] = df.body.apply(getSubjectivity)\n",
    "# df['BodyPolarity'] = df.body.apply(getPolarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984914b5",
   "metadata": {},
   "source": [
    "Here we transformed some features from continuous variables in to categorical variables so as to use them in our models.  The bins were chosen for the subjectivity and polarity based on the fact that they were probabilistic in the first place, leaving only the 'more sure' sides alone and setting the middle ground to a neutral state.  The bins for the length buckets were chosen based on quantiles which are shown in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_cut_labels = [0, 1, 2] #0 for objective, 1 for neutral, 2 for subjective\n",
    "subj_cut_bins = [0, 0.35, 0.65, 1]\n",
    "\n",
    "polar_cut_labels = [0, 1, 2] #0 for negative, 1 for neutral, 2 for positive\n",
    "polar_cut_bins = [-1, -0.5, 0.5, 1]\n",
    "\n",
    "len_cut_labels = [0, 1, 2, 3] #0 for shorter, 1 for average, 2 for longer, 3 for super long.  must be 1-hot-encoded in layers later\n",
    "len_cut_bins = [0, 0.004005, 0.01201202, 0.02002003, 1] #could just do a qcut here but keep as-is in case we change later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79145f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# df['ScaledBodyLen'] = scaler.fit_transform(df[['BodyLen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb633df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['SubjectivityBucket'] = pd.cut(df.BodySubjectivity, subj_cut_bins, labels = subj_cut_labels, include_lowest = True)\n",
    "# df['PolarityBucket'] = pd.cut(df.BodyPolarity, polar_cut_bins, labels = polar_cut_labels, include_lowest = True)\n",
    "# df['LengthBucket'] = pd.cut(df.ScaledBodyLen, len_cut_bins, labels = len_cut_labels, include_lowest = True) #remember these must be 1-hot encoded later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138b139",
   "metadata": {},
   "source": [
    "## EDA 1\n",
    "#### Begin with smaller subset (Jan-Apr)  \n",
    "Part of what I wanted to assess in my original project was if trends in the short term applied to the long term, and if smaller subsections of data could be used for quick results that matched up with the results of many times the data.  This spirit was carried to this section of the notebook where I took one third of my full dataset and attempted to model.  These results will be compared to the final, full dataset results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the smaller dataset\n",
    "janthruapr = df[(df['Date'].str.startswith('2020-01')) | (df['Date'].str.startswith('2020-02')) | \n",
    "   (df['Date'].str.startswith('2020-03')) | (df['Date'].str.startswith('2020-04'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any errors\n",
    "# janthruapr = janthruapr[~janthruapr['cComment'].isnull()]\n",
    "# janthruapr.reset_index(inplace = True, drop = True)\n",
    "#notneeded now that we just pull from full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show class distribution\n",
    "ax = janthruapr.BinaryBucket.value_counts().plot.pie(labels = [\">1\", \"<=1\"],\n",
    "                                                    autopct = '%1.1f%%', figsize = (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60259b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show raw score distribution\n",
    "ax = janthruapr.score.value_counts().sort_index(ascending = True).plot(kind = 'bar', rot = 0)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Freq.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zoom in on the majority of the scores\n",
    "ax = janthruapr[(janthruapr.score < 20) & (janthruapr.score > -20)].score.value_counts().sort_index(ascending = True).plot(kind = 'bar', rot = 0)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Freq.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show raw score quantiles\n",
    "janthruapr.score.quantile(q = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlations of features\n",
    "corr = janthruapr.corr()\n",
    "corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728eb02",
   "metadata": {},
   "source": [
    "From this we can see that there are unfortunately no big correlations present between the score - or it's representation as BinaryBucket - to any of the other features.  We just have to hope our models can pick up something that we can't see here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "janthruapr.BodyLen.quantile(q = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]) #clearly the largest is a bit of an ourlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218054f",
   "metadata": {},
   "source": [
    "Check out the top 10 most common words by frequency after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Counter()\n",
    "janthruapr['cComment'].str.lower().str.split().apply(results.update)\n",
    "y = [count for tag, count in results.most_common(10)]\n",
    "x = [tag for tag, count in results.most_common(10)]\n",
    "ax = plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(results.keys())} unique words in our cleaned text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca058e7",
   "metadata": {},
   "source": [
    "## Modeling 1\n",
    "#### Begin with smaller subset (Jan-Apr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20442e",
   "metadata": {},
   "source": [
    "The choices for our model architecture began by emulating Max Woolf's model which used only an input, embedding, and pooling layer before a sigmoid activation function for the comments branch.  This was then compared to quite a few other architectures such as having many dense layers, having just one additional dense layer, using bidrectional and single-way LSTMs, all of which attempted different pooling strategies and were compared while using different hyperparameters such as the word embeddings, etc.  \n",
    "All of these were compared until the best of them came out, and then of these best the simplest which achieved the same results were preferred.  It was believed before we started that a single LSTM layer could probably provide quite good results, and the truth was that this was correct.  A single layer LSTM followed by a sigmoid activation function provided results on par with extremely deep networks while fitting quickly.  Adding dropouts, poolings, and dense layers only added to the fitting time without affecting the validation AUC and accuracies much if at all.  \n",
    "The text embedding functions left below are only for reading in the Glove pre-trained text embeddings, but w2v and bag-of-words approaches were also tested to varying degrees of accuracy among our models.  Glove ended up giving the best, most consistent results and as such it is left in for our report.  \n",
    "##### A note on embeddings:  \n",
    "Glove being the best was expected since we chose the glove twitter pre-trained word vectors and we hypothesised that linguistic trends within twitter comments would probably be quite similar to linguistic trends within reddit comments.  Something unexpected was that the 25-day vectors were sufficient to capture all the information that our model could pull out.  Increasing the vector size to even the 200-day pre-trained word vectors had little to no impact on our models.  Since we were unsure, we did end up testing Glove twitter, w2v, and a word-of-bags approach on our models though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e689a22",
   "metadata": {},
   "source": [
    "First we have a few functions which prepare our glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1866da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_embedding(df, vocab_size, maxlen):\n",
    "    word_tokenizer = Tokenizer(vocab_size) #init tokenizer with the max features we want to see\n",
    "    word_tokenizer.fit_on_texts(df.cComment) #fit tokenizer on to our df's cleaned comments\n",
    "#     print(len(word_tokenizer.word_counts)) #not needed, nice to see\n",
    "    \n",
    "    comments_tf = word_tokenizer.texts_to_sequences(df.cComment)\n",
    "    comments_tf = pad_sequences(comments_tf, maxlen = maxlen)\n",
    "    return comments_tf, word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e40011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(path):#so we can change which embedding we use\n",
    "    embeddings = {} #empty, to be filled as we read in\n",
    "    with open(path, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vect = np.array(line_split[1:], dtype = float)\n",
    "            word = line_split[0]\n",
    "            embeddings[word] = vect\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(word_token, embeds, vocab_size):\n",
    "    weights = np.zeros((vocab_size +1, 25)) #25 as len(embeddings['example_text']) == 25\n",
    "    for word, i in word_token.word_index.items():\n",
    "        embedding = embeds.get(word)\n",
    "        if embedding is not None and i <= vocab_size:\n",
    "            weights[i] = embedding\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d07c15",
   "metadata": {},
   "source": [
    "Here we set some basic hyperparameters which we play with quite a bit in tuning our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000 #set max # features to 50k for now\n",
    "glove25_path = \"GloveTwitterData\\\\glove.twitter.27B.25d.txt\"\n",
    "glove50_path = \"GloveTwitterData\\\\glove.twitter.27B.50d.txt\"\n",
    "glove100_path = \"GloveTwitterData\\\\glove.twitter.27B.100d.txt\"\n",
    "glove200_path = \"GloveTwitterData\\\\glove.twitter.27B.200d.txt\"\n",
    "embedding_dims = 25 #set from GLOVE\n",
    "maxlen = 50 #may want to lower, 19 keeps entirety of 90% of our comments but seems a bit long.\n",
    "batch_size = 256 #can change as needed\n",
    "date_dims = 64 #dims for our date layers in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10774db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_tf_funny, word_tokenizer_funny = prep_embedding(janthruapr, vocab_size, maxlen)\n",
    "embeddings_funny = get_embeddings(glove25_path) \n",
    "weights_funny = get_weights(word_tokenizer_funny, embeddings_funny, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd7fc",
   "metadata": {},
   "source": [
    "This cell below defines our model.  Right now this is only showing a very basic LSTM(50) with a 20% dropout (labelled as lstm50_0dense in saved files) as an example.  The formatting for this model and the way that the features are called later are due to Max Woolf's influence.  When comparing our model(s) to his from his paper it was easier to use this formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a83660",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_input_layer = Input(shape = (maxlen,), name = 'comment_input')\n",
    "comments_embedding_layer = Embedding(vocab_size +1, embedding_dims, weights = [weights_funny])(comments_input_layer)\n",
    "comments_lstm_layer = LSTM(50, dropout = 0.2)(comments_embedding_layer)\n",
    "\n",
    "output = Dense(1, activation = 'sigmoid', name = 'output')(comments_lstm_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = [comments_input_layer], outputs = [output])\n",
    "\n",
    "#loss_weights are how much we weightcomments[0th] vs. the date info [1st].  Set to 1:1 for first pass\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = Adam(0.0005), metrics = ['accuracy', 'AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycallbacks = [EarlyStopping(monitor = 'val_loss', patience = 3)] #basic early stopping if we get 3 bad epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_janthruapr, test_janthruapr = train_test_split(janthruapr, test_size = 0.2) #split our dataset\n",
    "\n",
    "comments_train = comments_tf_funny[train_janthruapr.index, :] #get the comments we'll use\n",
    "binarybucket_train = train_janthruapr.BinaryBucket[train_janthruapr.index] #get the y values\n",
    "\n",
    "with tf.device('/GPU:0'): #use my GPU\n",
    "    start = dt.datetime.now()\n",
    "    history = model.fit(comments_train, binarybucket_train,\n",
    "                   batch_size = batch_size, epochs = 25, validation_split= 0.2, callbacks = mycallbacks)\n",
    "    end = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5aed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize = (15,10))\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(history.epoch, history.history['loss'], label = 'Training Loss')\n",
    "axs[0].plot(history.epoch, history.history['val_loss'], label = 'Validation Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Comment Accuracy')\n",
    "axs[1].plot(history.epoch, history.history['accuracy'], label = 'Comment Training Accuracy')\n",
    "axs[1].plot(history.epoch, history.history['val_accuracy'], label = 'Comment Validation Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].set_title('Comment AUC')\n",
    "axs[2].plot(history.epoch, history.history['auc'], label = 'Comment Training AUC')\n",
    "axs[2].plot(history.epoch, history.history['val_auc'], label = 'Comment Validation AUC')\n",
    "axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest val acc was: {round(np.max(history.history['val_accuracy']),3)}\")\n",
    "print(f\"The highest val AUC was: {round(np.max(history.history['val_auc']),3)}\")\n",
    "print(f\"Fitting took {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6121cf8",
   "metadata": {},
   "source": [
    "Baseline here was ~56.2% acc from our largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c98608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model images\n",
    "# plot_model(model, to_file='comment_only_dense50.png')\n",
    "# plot_model(model, to_file='comment_only_dense50_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c0be6",
   "metadata": {},
   "source": [
    "## Results 1\n",
    "#### Begin with smaller subset (Jan-Apr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf950cf",
   "metadata": {},
   "source": [
    "The above type of procedure was ran through many, many times.  The hyperparameters we tested included the embedding type, the optimizer (and the hyperparameters within a given optimizer), the features used, maxlen, batch size, and of course the model architecture itself.  \n",
    "While there was quite a bit of time going over this, it was unfortunately my last step to remove the cleaned comments feature itself and assess the model.  This was unfortunate due to the fact that what I had thought was a great model to predict the sucess of a post ended up being just a great model of the API's way of calling in my data!  I was getting accuracies of over 80% when using the 'Hour' and 'Day_of_Year' features.  The fact that they were significant at all should have, in hindsight, immediately alerted me to the issue as there was no reasonable way for the day of the year to matter when looking at only this tiny subset of data.  Unfortunately this meant that I wasted a lot of time assessing models which only modelled the way the API chose to pick its data.  The plot below shows how this 80% accuracy was achieved with just the day of year information due to the way the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1124b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = {}\n",
    "for day in janthruapr.Day_of_Year.unique():\n",
    "    bucket[day] = np.mean(janthruapr[janthruapr.Day_of_Year == day].BinaryBucket)\n",
    "lists = sorted(bucket.items())\n",
    "x, y = zip(*lists)\n",
    "plt.bar(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56235b4",
   "metadata": {},
   "source": [
    "Because of this issue a lot of work was lost and it was revealed that the true model accuracy was around 61% at best.  Some of the remaining model outcomes are reported below.  The 'Model' column represents the name of the saved model image.  Some of these images are located in the 'miscImages' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b158b",
   "metadata": {},
   "source": [
    "|Model|Embedding|Optimizer|Features|MaxValAcc|MaxValAUC|TimeToFit|maxlen|batchsize|\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "|commentonly_original|glove200|adam|cComment|0.606|0.624|00:30|50|256|\n",
    "|commentonly_original|w2v|adam|cComment|0.600|0.605|00:50|50|256|\n",
    "|commentonly_original|BOW|adam|cComment|0.567|0.583|01:01|50|256|\n",
    "|commentonly_original|glove200|sgd|cComment|0.605|0.622|13:21|50|256|\n",
    "|commentonly_original|glove25|adam|cComment|0.606|0.622|00:29|50|256|\n",
    "|commentonly_original|glove25|adam|cComment|0.604|0.624|00:50|20|256|\n",
    "|commentonly_original|glove25|Adam(0.001)|cComment|0.607|0.622|01:23|50|256|\n",
    "|commentonly_original|glove25|Adam(0.0005)|cComment|0.607|0.623|01:11|50|256|\n",
    "|commentonly_lstm50_3dense|glove25|Adam(0.0005)|cComment|0.612|0.629|01:33|50|256|\n",
    "|commentonly_lstm50_2dense|glove25|Adam(0.0005)|cComment|0.611|0.631|01:21|50|256|\n",
    "|commentonly_lstm100_3dense|glove25|Adam(0.0005)|cComment|0.609|0.624|01:45|50|256|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d65fd",
   "metadata": {},
   "source": [
    "The first results here use the 'commentonly_original' model which was Max Woolf's mixed model with the date inputs removed.  This same model was used to find out if the embedding type influenced the model performance.  I found my best performance with pre-trained glove embeddings and it was noted that the size of the embeddings used didn't influence performance on this dataset.  Next I looked at different types of optimizers and it was found that adam and sgd performed the best with sgd giving almost identical scores to adam with much nicer loss curves, but at the cost of being much, much slower.  As such the adam optimizer then had its initial learning rate tuned a bit and it was found to give the best results with an initial LR of 0.0005 on average.  Finally a few different model architectures were attempted.  The 'original' refers to the model architecture from the influential Max Woolf reddit prediction model.  It was found to be similar, but a bit behind some of the lstm-based architectures that were attempted here. The maxlen and batchsize parameters were found in some of the many, many other trials that are not listed above.  \n",
    "The models that ended up being the best here seemed to be tied between commentonly_original and commentonly_lstm50_0dense.  The reason that they appear to be tied is that while the commentonly_original gave a Validation Accuracy of 0.606 and Validation AUC of 0.624, it fit in only 30 seconds.  Compared to the commentonly_lstm50_0dense which acheived a Validation Accuracy of 0.61 and Validation AUC of 0.63 but fitting in 1:10 it is hard to pick.  I think that overall the commentonly_original has a chance at beating out the LSTM just due to it's speed, but I would like to compare these more with the full dataset before I decide which is best overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16987b2",
   "metadata": {},
   "source": [
    "## EDA 2\n",
    "#### Full 2020 Dataset  \n",
    "We repeat a lot of the basic EDA done in our smaller subset here and find great agreement between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.BinaryBucket.value_counts().plot.pie(labels = [\">1\", \"<=1\"],\n",
    "                                                    autopct = '%1.1f%%', figsize = (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb550a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.score.value_counts().sort_index(ascending = True).plot(kind = 'bar', rot = 0)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Freq.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[(df.score < 10) & (df.score > -10)].score.value_counts().sort_index(ascending = True).plot(kind = 'bar', rot = 0)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Freq.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f712f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.quantile(q = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad08bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BodyLen.quantile(q = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]) #clearly the largest is a bit of an ourlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BodyLen.quantile(q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]).plot(kind = 'bar') #plot just 0-90th percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04aff5",
   "metadata": {},
   "source": [
    "## Modeling 2\n",
    "#### Full 2020 Dataset\n",
    "Since this section basically just re-does the modeling above it is commented out for now.  It is left in to show exactly how the modeling was done if any attempts to re-create it are done (and for possible future work).  It also shows how the multi-input model(s) were built.  \n",
    "It was again assumed that RNNs made from LSTM layers would perform the best, and it was found that a single LSTM again performed on par with other, more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db18bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 50000 #set max # features to 50k for now\n",
    "# embedding_dims = 25 #set from GLOVE\n",
    "# maxlen = 50 #may want to lower, 19 keeps entirety of 90% of our comments but seems a bit long.\n",
    "# batch_size = 256 #can change as needed\n",
    "# date_dims = 64 #dims for our date layers in our model\n",
    "# onehot_dims = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93663f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_tf_funny, word_tokenizer_funny = prep_embedding(df, vocab_size, maxlen)\n",
    "# embeddings_funny = get_embeddings(glove25_path) \n",
    "# weights_funny = get_weights(word_tokenizer_funny, embeddings_funny, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707864d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputComment = Input(shape = (maxlen,))\n",
    "# input_subjectivity = Input(shape = (1,))\n",
    "# input_polarity = Input(shape = (1,))\n",
    "# input_length = Input(shape = (1,)) \n",
    "\n",
    "# #cComments\n",
    "# x = Embedding(vocab_size +1, embedding_dims, weights = [weights_funny])(inputComment)\n",
    "# x = LSTM(50, dropout = 0.2)(x)\n",
    "# x = Dense(1, activation = 'sigmoid')(x)\n",
    "# x = Model(inputs = inputComment, outputs = x)\n",
    "\n",
    "# #SubjectivityBucket\n",
    "# y = CategoryEncoding(num_tokens = 3, output_mode = 'one_hot')(input_subjectivity)\n",
    "# # y = Reshape((onehot_dims,))(y)\n",
    "# y = Model(inputs = input_subjectivity, outputs = y)\n",
    "\n",
    "# #PolarityBucket\n",
    "# z = CategoryEncoding(num_tokens = 3, output_mode = 'one_hot')(input_polarity)\n",
    "# # z = Reshape((onehot_dims,))(z)\n",
    "# z = Model(inputs = input_polarity, outputs = z)\n",
    "\n",
    "# #LengthBucket\n",
    "# p = CategoryEncoding(num_tokens = 4, output_mode = 'one_hot')(input_length)\n",
    "# # p = Reshape((onehot_dims,))(p)\n",
    "# p = Model(inputs = input_length, outputs = p)\n",
    "\n",
    "# combined = concatenate([x.output, y.output, z.output, p.output])\n",
    "# w = Dense(11, activation = 'relu')(combined)\n",
    "# w = Dense(1, activation = 'sigmoid')(w)\n",
    "\n",
    "# model = Model(inputs = [x.input, y.input, z.input, p.input], outputs = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b93cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'AUC'])\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "# # comments_tr\n",
    "# comments_train = comments_tf_funny[train_df.index, :]\n",
    "# subjectivity_train = train_df.SubjectivityBucket[train_df.index]\n",
    "# polarity_train = train_df.PolarityBucket[train_df.index]\n",
    "# length_train = train_df.LengthBucket[train_df.index]\n",
    "# binarybucket_train = train_df.BinaryBucket[train_df.index]\n",
    "\n",
    "# with tf.device('/GPU:0'): #use my GPU\n",
    "#     start = dt.datetime.now()\n",
    "#     history = model.fit([comments_train, subjectivity_train, polarity_train, length_train], binarybucket_train,\n",
    "#                    batch_size = batch_size, epochs = 100, validation_split= 0.2, callbacks = mycallbacks)\n",
    "#     end = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 3, figsize = (15,10))\n",
    "# axs[0].set_title('Loss')\n",
    "# axs[0].plot(history.epoch, history.history['loss'], label = 'Training Loss')\n",
    "# axs[0].plot(history.epoch, history.history['val_loss'], label = 'Validation Loss')\n",
    "# axs[0].legend()\n",
    "\n",
    "# axs[1].set_title('Comment Accuracy')\n",
    "# axs[1].plot(history.epoch, history.history['accuracy'], label = 'Comment Training Accuracy')\n",
    "# axs[1].plot(history.epoch, history.history['val_accuracy'], label = 'Comment Validation Accuracy')\n",
    "# axs[1].legend()\n",
    "\n",
    "# axs[2].set_title('Comment AUC')\n",
    "# axs[2].plot(history.epoch, history.history['auc'], label = 'Comment Training AUC')\n",
    "# axs[2].plot(history.epoch, history.history['val_auc'], label = 'Comment Validation AUC')\n",
    "# axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The highest val acc was: {round(np.max(history.history['val_accuracy']),3)}\")\n",
    "# print(f\"The highest val AUC was: {round(np.max(history.history['val_auc']),3)}\")\n",
    "# print(f\"Fitting took {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='comPolSubLen2.png')\n",
    "# plot_model(model, to_file='comPolSubLen2_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc7411",
   "metadata": {},
   "source": [
    "## Results 2\n",
    "#### Full 2020 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8ed43",
   "metadata": {},
   "source": [
    "(Maxlen and batchsize were also tested here but found to still perform best at 50 and 256 respectively and as such are not included in the chart.)  \n",
    "Remember that the baseline is now 56.0% (0.2% lower than before)\n",
    "\n",
    "|Model|Embedding|Optimizer|Features|MaxValAcc|MaxValAUC|TimeToFit|\n",
    "|--|--|--|--|--|--|--|\n",
    "|commentonly_original|glove200|adam|cComment|0.601|0.617|02:54|\n",
    "|commentonly1|glove200|adam|cComment|0.603|0.618|03:36|\n",
    "|commentonly1|glove200|adam|cComment|0.608|0.626|02:38|\n",
    "|commentonly2|glove200|adam|cComment|0.609|0.626|02:03|\n",
    "|commentonly2|glove25|adam|cComment|0.609|0.626|02:39|\n",
    "|commentonly7|glove25|adam|cComment|0.610|0.627|03:13|\n",
    "|comPolSubLen1|glove25|adam|cComment, SubjectivityBucket, PolarityBucket, LengthBucket|0.61|0.626|03:22|\n",
    "|comPolSubLen5|glove25|adam|cComment, SubjectivityBucket, PolarityBucket, LengthBucket|0.609|0.626|03:18|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d996c",
   "metadata": {},
   "source": [
    "Using the full dataset we see that the addition of Polarity, Subjectivity, and Length have seemingly no effect, at least with the buckets that were tested for these models.  Using only our cleaned comments gave similar results and fitted a bit quicker.  Noting that commentonly2 involved just one LSTM layer (with a 20% dropout as a LSTM hyperparameter), we see that again a simple model performs the best.  Adding on more layers served only to increase fitting time while leaving our Validation Accuracy and Validation AUC values stagnant.  \n",
    "One other metric that we can use to evaluate our models (other than the accuracy, AUC, complexity, and time to fit) that we haven't touched on much yet is the loss curve plots produced from our modeling routine.  We have attempted so many models that it would be combersome to include all the outputs, but we can draw attention to one interesting facet of this criteron here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = mpimg.imread(\"miscImages/commentonly5_adam1e-4_better_curve_example.png\")\n",
    "img1 = mpimg.imread(\"miscImages/commentonly5_adamDefault_worse_curve_example.png\")\n",
    "img2 = mpimg.imread(\"miscImages/commentonly5_sgdDefault_great_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize = (10,35))\n",
    "axs[0].set_title(\"Adam(1e-4)-commentonly5\")\n",
    "axs[0].imshow(img0)\n",
    "axs[1].set_title(\"\\'adam\\'(default)-commentonly5\")\n",
    "axs[1].imshow(img1)\n",
    "axs[2].set_title(\"\\'sgd\\'(default)-commentonly5\")\n",
    "axs[2].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b311c4",
   "metadata": {},
   "source": [
    "Above we show three loss curves associated with three different optimizers used on the same model.  When choosing our best model(s) we also looked at these loss curves and assessed the best of them.  Clearly in this case when comparing an Adam(1e-4) (top), 'adam'(default) (middle), and 'sgd'(default) (bottom), we find that the best loss curve comes from the Adam(1e-4) model here since the 'sgd'(default) result is both unrealistic looking and leads to worse model performance over a longer period of time.  Of course that isn't to say that this was the absolute best loss curve we found, but it does illustrate some of the variations that we often saw when comparing loss curves within models but between optimizers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f59fbc",
   "metadata": {},
   "source": [
    "Our best model for the full dataset was labeled as 'commentonly2' and it had the following model architecture and loss curve with our best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, figsize = (15, 15))\n",
    "axs[0].set_title(\"commentonly2 Architecture\")\n",
    "axs[0].imshow(mpimg.imread('miscImages/commentonly2_shapes.png'))\n",
    "axs[1].set_title(\"commentonly2 Loss Curve\")\n",
    "axs[1] = plt.imshow(mpimg.imread('miscImages/commentonly2_loss_example.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89e63",
   "metadata": {},
   "source": [
    "When using the glove25 embeddings and an 'adam'(default) optimizer it resulted in a maximum validation accuracy of 0.609 and a maximum validation AUC of 0.626.  The fitting time took only 2:39.  The loss curve was relatively nice with a slight upturn at the end.  Adding in more complex optimization routines such as having the LR decrease on plateau via an early stopping routine produced similar results in accuracy/AUC but with a large increase in fitting times for only marginally better loss curves.  As such this is our chosen final, best model for the full dataset and it used only the 'cComments' feature to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d5a27",
   "metadata": {},
   "source": [
    "## Comparison to non deep learning method\n",
    "#### Full 2020 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1299997",
   "metadata": {},
   "source": [
    "Finally before our conclusion we attempt a non-DL method to see just how much 'extra' information our DL model pulls from the comments as compared to a relatively less complex approach.  The actual results are not printed here as it took quite a bit to go through all of the gridsearch results, but the final results are included as an image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78839fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first try a SGDClassifier with just the text data\n",
    "# train_df, test_df = train_test_split(df, test_size = 0.2)\n",
    "# comments_train = comments_tf_funny[train_df.index, :]\n",
    "# binarybucket_train = train_df.BinaryBucket[train_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55356b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting Fitting\")\n",
    "# start = dt.datetime.now()\n",
    "# clf = SGDClassifier().fit(comments_train, binarybucket_train)\n",
    "# end = dt.datetime.now()\n",
    "# print(f\"Fitting Took {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28354d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_test = comments_tf_funny[test_startbig.index, :]\n",
    "# predicted = clf.predict(comments_test)\n",
    "# binarybucket_test = test_startbig.BinaryBucket[test_startbig.index]\n",
    "# acc = np.mean(predicted == binarybucket_test)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add768ae",
   "metadata": {},
   "source": [
    "With gridsearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', SGDClassifier())])\n",
    "\n",
    "# params = {'vect__ngram_range': [(1,1), (1,2)],\n",
    "#          'tfidf__use_idf': (True, False),\n",
    "#          'clf__alpha': (1e-1, 1e-2, 1e-3, 1e-4, 1e-5)}\n",
    "\n",
    "# train_startbig, test_startbig = train_test_split(startbig, test_size = 0.2)\n",
    "# comments_train_clf = train_startbig.cComment[train_startbig.index]\n",
    "# binarybucket_train_clf = train_startbig.BinaryBucket[train_startbig.index]\n",
    "\n",
    "\n",
    "# gridsearch_clf = GridSearchCV(text_clf, params, scoring = ['accuracy', 'roc_auc'], cv = 5, verbose = 2, refit = 'accuracy')\n",
    "# gridsearch_clf = gridsearch_clf.fit(comments_train_clf, binarybucket_train_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220359a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gridsearch_clf.best_score_, gridsearch_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9913e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(\"gridsearch_results.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1e519",
   "metadata": {},
   "source": [
    "Clearly our deep learning model captures more information from our cleaned up comments- and it's much faster too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99d9f8",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ac8ad",
   "metadata": {},
   "source": [
    "In this project we have found a slightly successful model to predict if a reddit comment in the 'funny' subreddit in 2020 would succeed or fail.  From our brief dive in to the applicability of this model to larger timeframes we see that models which succeeded at a smaller timeframe also performed well at a larger timeframe, although a bit below optimally.  We also see that the maximum validation AUC and accuracy remain relatively stable even when tripling the amount of data that the model receives.  This seems to imply that our model has gotten just about all the information it can out of these cleaned up comments and it is up to us to find or create additional features to improve our model performance.  \n",
    "Something that would benefit our model greatly would be to revisit the way that we procured our data with the Pushshift API and attempt to find a way to pull in data more evenly allowing for some in depth analysis on the time data that we pulled.  This would allow us to model possible temporal trends and add these temporal features to our models.  \n",
    "Another feature that would be quite interesting relates to my original project proposal which included creating a model for the sentiment of a given comment within a given subreddit and to use this comment sentiment, along with the sentiment of the post, and their possible differences, as features.  I was not sure which would give the greatest impact, but I believe that this has a definite possibility to allow for future insights.  For example, I would expect a positive comment on a positive post in 'funny' to, on average, get more karma than say a positive comment on a negative post in 'news'.  The tones of the subreddits are just different.  \n",
    "\n",
    "Future work that doesn't involve a whole new project could be to incorporate additional subreddits in to our current modeling procedures and to see if there are trends between subreddits.  Another thing could be to simply add in more data.  Or perhaps adding in more, custom stopwords in addition to the default stopwords we already included could allow us to get more information out of our cleaned comments without us having to even do any more modeling.  \n",
    "\n",
    "Overall this project serves as a very light dip in to the analysis of reddit comments.  I believe that the best way to carry on this project in the future would be to look in to the way the data is pulled and get useable time-related information.  With what we have already done we have a basic outline of many hyperparameters that work and ones which don't which would streamline future modeling choices and vastly reduce the time it would take to test new architectures and hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
